{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive plotting\n",
    "# %matplotlib widget\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import linkage, leaves_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = \".../Summary_Agenda_Daily.csv\"\n",
    "#\".../Summary_Agenda__1 hours__2023-01-01 00:00 +0000__2024-06-01 00:00 +0000.csv\"\n",
    "# \".../Summary_Agenda__4 hours__2023-01-01 00:00 +0000__2024-06-01 00:00 +0000.csv\"\n",
    "# \".../Summary_Agenda_Daily.csv\"\n",
    "# \".../Summary_Agenda_Daily.csv\"\n",
    "# skip the first column (date indication)\n",
    "rawDF = pd.read_csv(FILE, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_columns_by_tag(raw, columns_dict, split_string = \" \", keep_columns = [\"from_time\", \"to_time\"]):\n",
    "    \"\"\"Produces a dataset by combining all columns with a tag from the selection into one, over multiple.\\n\n",
    "    - raw: DataFrame containing the keep_columns\n",
    "    - columns_dict: associates column name to the sum of all columns containing any string\n",
    "    -   \\n\"\"\"\n",
    "    if not all([kept_column in raw.columns for kept_column in keep_columns]):\n",
    "        raise KeyError(\"keep_columns is defined but some of its elements are not in raw.columns\")\n",
    "\n",
    "    original_columns = raw.columns.to_series()\n",
    "\n",
    "    # creates a new dataframe from the keys of the dictionary\n",
    "    dataframe = pd.DataFrame(columns= keep_columns + list(columns_dict.keys()))\n",
    "    if \"from_time\" in keep_columns:\n",
    "        dataframe['from_time'] = pd.to_datetime(raw['from_time'])\n",
    "    if \"to_time\" in keep_columns:\n",
    "        dataframe['to_time'] = pd.to_datetime(raw['to_time'])\n",
    "\n",
    "    for key in columns_dict.keys():\n",
    "        selection = original_columns.str.split(split_string).map(lambda x: any([f in x for f in columns_dict[key]]))\n",
    "        dataframe[key] = raw.loc[:, selection].sum(axis=1)\n",
    "\n",
    "    return dataframe\n",
    "    \n",
    "    \n",
    "# note that the tags should be exclusive to avoid double counting\n",
    "tag_tree = {\n",
    "    \"standard\": {\n",
    "        \"Sleep\": [\"SWO\", \"SFR\"],\n",
    "        \"Lessons\": [\"LES\"],\n",
    "        \"Revision\": [\"REV\", \"EXM\"],\n",
    "        \"Repetitive\": [\"BUR\", \"WRK\", \"TDY\", \"ORG\"],\n",
    "        \"Projects\": [\"PRJ\"],\n",
    "        \"Media\": [\"MDI\"],\n",
    "        \"Social\": [\"CAL\", \"OUT\", \"EVE\", \"DOG\"],\n",
    "    },\n",
    "    \"study\": {\n",
    "        \"Theory\": [\"R\"],\n",
    "        \"Exercise\": [\"E\"],\n",
    "        \"Projects\": [\"P\"],\n",
    "        \"Exams\": [\"EXM\"],\n",
    "        \"Lessons\": [\"LES\"],\n",
    "    },\n",
    "    \"extended\": {\n",
    "        \"Sleep\": [\"SWO\", \"SFR\"],\n",
    "        \"Lessons\": [\"LES\"],\n",
    "        \"Revision\": [\"REV\"],\n",
    "        \"Exams\": [\"EXM\"],\n",
    "        \"Repetitive\": [\"REP\"],\n",
    "        \"Bureaucracy\": [\"BUR\"],\n",
    "        \"Work\": [\"WRK\"],\n",
    "        \"Tidying\": [\"TDY\"],\n",
    "        \"Organization\": [\"ORG\"],\n",
    "        \"Projects\": [\"PRJ\"],\n",
    "        \"Media\": [\"MDI\"],\n",
    "        \"Calls\": [\"CAL\"],\n",
    "        \"Going Out\": [\"OUT\", \"EVE\", \"DOG\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping = \"standard\"\n",
    "\n",
    "df = sum_columns_by_tag(rawDF, tag_tree[grouping])\n",
    "\n",
    "print(df.head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timedelta(td):\n",
    "    days = td.days\n",
    "    seconds = td.seconds\n",
    "    hours, remainder = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(remainder, 60)\n",
    "\n",
    "    parts = []\n",
    "    if days > 0:\n",
    "        parts.append(f\"{days} days\")\n",
    "    if hours > 0:\n",
    "        parts.append(f\"{hours} hours\")\n",
    "    if minutes > 0:\n",
    "        parts.append(f\"{minutes} minutes\")\n",
    "    if seconds > 0:\n",
    "        parts.append(f\"{seconds} seconds\")\n",
    "\n",
    "    return ', '.join(parts)\n",
    "\n",
    "# extracts the difference in time between the first two rows\n",
    "# WARNING: assumes all differences are equal\n",
    "time_step = df['from_time'][1] - df['from_time'][0]\n",
    "time_step_string = format_timedelta(time_step)\n",
    "print(\"Delta\", time_step_string)\n",
    "\n",
    "selection = grouping + \" \" + time_step_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cannot cross the year boundary\n",
    "period_dates = {\n",
    "    \"lessons\": [(\"-09-01\", \"-12-01\"), (\"-02-01\", \"-05-20\")],\n",
    "    \"exams\": [(\"-12-01\", \"-12-31\"), (\"-01-01\", \"-02-01\"), (\"-05-20\", \"-06-20\")],\n",
    "    \"university\": [(\"-09-01\", \"-12-31\"), (\"-01-01\", \"-06-20\")],\n",
    "}\n",
    "\n",
    "def get_df_period_mask(period_dates_string, dataframe):\n",
    "    dates = period_dates[period_dates_string]\n",
    "    \n",
    "    for dates_tuple in dates:\n",
    "        from_date = pd.to_datetime(dataframe['from_time'].dt.year.astype(str) + dates_tuple[0], utc=True)\n",
    "        to_date = pd.to_datetime(dataframe['from_time'].dt.year.astype(str) + dates_tuple[1], utc=True)\n",
    "        mask = (dataframe[\"from_time\"] >= from_date) & (dataframe[\"from_time\"] < to_date)\n",
    "        if dates_tuple == dates[0]:\n",
    "            total_mask = mask\n",
    "        else:\n",
    "            total_mask = total_mask | mask\n",
    "    \n",
    "    return total_mask\n",
    "  \n",
    "def extract_data_from_df(dataframe, columns = [\"from_time\", \"to_time\"]):\n",
    "    \"\"\"Extracts the data from the dataframe according to the selection.\"\"\"\n",
    "    return dataframe.drop(columns=columns)\n",
    "    \n",
    "def to_normalised_df(dataframe_of_only_data):\n",
    "    \"\"\"Normalises the data in the dataframe.\"\"\"\n",
    "    return pd.DataFrame(StandardScaler().fit_transform(dataframe_of_only_data), columns=dataframe_of_only_data.columns, index=dataframe_of_only_data.index) \n",
    "\n",
    "def extend_dataframe_with_period(dataframe, period_dates_string):\n",
    "    \"\"\"Extends the dataframe with the period mask.\"\"\"\n",
    "    dataframe[period_dates_string] = get_df_period_mask(period_dates_string, dataframe)\n",
    "    return dataframe\n",
    "\n",
    "def get_all_period_masks(dataframe):\n",
    "    period_masks = {}\n",
    "    for period in period_dates.keys():\n",
    "        period_masks[period] = get_df_period_mask(period, dataframe)\n",
    "                \n",
    "    return period_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_feature_histogram(dataframe):\n",
    "    # removes the zero values from each column, allowing for a more natural scale of the dataset  \n",
    "    plt.figure()\n",
    "    axes = dataframe.apply(lambda x: x[x != 0]).hist(figsize=(15, 7))\n",
    "    plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xlabel(f\"Total duration in steps of {time_step_string}\")\n",
    "        ax.set_ylabel(\"Occurrences\")\n",
    "    plt.savefig(f'fil/Histogram Columns {selection}.png') \n",
    "    \n",
    "data = extract_data_from_df(df)\n",
    "produce_feature_histogram(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# days without any activity\n",
    "sparsity = data.apply(lambda x: x[x==0]).count()/len(data)\n",
    "plt.figure()\n",
    "sparsity.sort_values(ascending=False).plot(kind='bar', figsize=(7, 4), title= time_step_string + \" steps without any activity\", grid=True)\n",
    "plt.savefig(f'fil/Sparsity Chart {selection}.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "data.sum(axis=1).hist(figsize=(5, 4))\n",
    "plt.xlabel(f\"Total duration per step of {time_step_string}\")\n",
    "plt.ylabel(\"Number of occurrences\")\n",
    "plt.title(\"Histogram of the total duration per step\")\n",
    "plt.savefig(f'fil/Histogram Total {selection}.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxes = pd.DataFrame(columns=[\"max\"] + list(df.columns))\n",
    "\n",
    "m = []\n",
    "\n",
    "for col in data.columns:\n",
    "    max_idx = np.argmax(data[col]) \n",
    "\n",
    "    m.append(pd.Series([col] + df.iloc[max_idx].tolist(), index=maxes.columns))\n",
    "\n",
    "maxes = pd.concat(m, axis=1).T.reset_index(drop=True)\n",
    "\n",
    "print(\"Possible outliers in the data\")\n",
    "print(maxes.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data in range\\n{df['from_time'].min()}\\n{df['from_time'].max()}\")\n",
    "\n",
    "# normalises the data\n",
    "scaled = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pca(x, display=False):\n",
    "    \"\"\"Performs PCA with n_components, and returns them\"\"\"\n",
    "    n_components = len(x.columns)\n",
    "    # Create a PCA instance\n",
    "    pca = PCA(n_components = n_components)\n",
    "\n",
    "    # Fit the data\n",
    "    components = pca.fit_transform(x)\n",
    "    rounded_components = np.vectorize(lambda x: f\"{x:.2f}\")(pca.explained_variance_ratio_.cumsum())\n",
    "    print(\"Cumulative sum of the principal components\", rounded_components)\n",
    "\n",
    "    if display:\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, align='center')\n",
    "        plt.step(range(1, len(explained_variance_ratio) + 1), np.cumsum(explained_variance_ratio), where='mid', color='red', label='Cumulative Explained Variance')\n",
    "        plt.ylabel('Explained Variance Ratio')\n",
    "        plt.xlabel('Principal Components')\n",
    "        plt.title('PCA % Explained Variance Ratio')\n",
    "        plt.xticks(range(1, len(explained_variance_ratio)))\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'fil/PCA 2D Barplot-Histogram Total {selection}.png') \n",
    "\n",
    "    return pd.DataFrame(data=components, columns=[f\"PC{i}\" for i in range(1, n_components + 1)])\n",
    "\n",
    "p_comp = perform_pca(data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_association(scaled_data, num_clusters):\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=num_clusters, linkage=\"ward\")\n",
    "    return hierarchical_cluster.fit_predict(scaled_data)\n",
    "\n",
    "NUMBER_OF_CLUSTERS = 5\n",
    "df_cl = df.copy()\n",
    "df_cl[\"cluster\"] = get_cluster_association(scaled, NUMBER_OF_CLUSTERS)\n",
    "\n",
    "cluster_grouping = df_cl.drop(columns=[\"from_time\", \"to_time\"]).groupby(\"cluster\")\n",
    "cluster_sizes = cluster_grouping.size()\n",
    "total_size = len(df_cl)\n",
    "\n",
    "print(\"Size of the clusters\")\n",
    "for i, size in cluster_sizes.items():\n",
    "    float_value = size / total_size\n",
    "    print(f\"+ Cluster {i} :: {float_value:.0%}, {size} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean daily time expenditure in minutes, per cluster (row) and activity (col)\")\n",
    "means = cluster_grouping.mean()\n",
    "print(means.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_notable_values(dataframe):\n",
    "    return dataframe.apply(lambda col: col.rank(method='min', ascending=False).astype(int) - 1)\n",
    "\n",
    "def z_score_columns(dataframe):\n",
    "    \"\"\"Computes a z-score normalisation over the columns of a DataFrame\"\"\"\n",
    "    return dataframe.apply(lambda col: (col - col.mean()) / col.std())\n",
    "\n",
    "z_normalized = z_score_columns(means)\n",
    "\n",
    "def assign_significance_value_lambda(x, thresh):\n",
    "    if x >= thresh:\n",
    "        return 1\n",
    "    elif x < -thresh:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def highlight_significant(z_normalized, thresh):\n",
    "    assert thresh > 0, \"Threshold must be positive\"\n",
    "    return z_normalized.map(lambda x : assign_significance_value_lambda(x, thresh))\n",
    "\n",
    "def print_significant(significant):\n",
    "    dataframe = pd.DataFrame(columns=[\"High\", \"Low\"])\n",
    "    for index, row in significant.iterrows():\n",
    "        high = [column for column, value in row.items() if value == 1]\n",
    "        low = [column for column, value in row.items() if value == -1]\n",
    "        dataframe.loc[index] = [str(high), str(low)]\n",
    "    return dataframe\n",
    "\n",
    "def analyze_significance(mean_time_expenditure, thresh):\n",
    "    \"\"\"Normalises (z-score) the dataframe over the columns, then identifies the values\n",
    "    with sdev above thresh or below -thresh\"\"\"\n",
    "    return print_significant(highlight_significant(z_score_columns(mean_time_expenditure), thresh))\n",
    "\n",
    "cluster_meaning = analyze_significance(means, 1)\n",
    "print(cluster_meaning.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_diagonal_na(dataframe):\n",
    "    copy = dataframe.copy()\n",
    "    np.fill_diagonal(copy.values, np.nan)\n",
    "    return copy\n",
    "\n",
    "def get_corr_matrix(data_dataframe, method):\n",
    "    correlation = data_dataframe.corr(method=method)\n",
    "    correlation_matrix_without_self = fill_diagonal_na(correlation)\n",
    "\n",
    "    linked = linkage(correlation, 'ward')\n",
    "\n",
    "    # Reorder the columns based on the clustering\n",
    "    ordered_columns = correlation_matrix_without_self.columns[leaves_list(linked)]\n",
    "\n",
    "    # Display the reordered correlation matrix\n",
    "    reordered_corr = correlation_matrix_without_self.loc[ordered_columns, ordered_columns]\n",
    "\n",
    "    return reordered_corr\n",
    "\n",
    "def show_correlation_matrix_method_comparisons(data_dataframe):\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5), gridspec_kw={'width_ratios': [0.75, 1]})\n",
    "\n",
    "    sns.heatmap(get_corr_matrix(data_dataframe, \"pearson\"), annot=False, cmap='coolwarm', fmt=\"\", linewidths=1, ax=axes[0], square=True, cbar=False)\n",
    "    axes[0].set_title('Pearson Correlation')\n",
    "\n",
    "    sns.heatmap(get_corr_matrix(data_dataframe, \"kendall\"), annot=False, cmap='coolwarm', fmt=\"\", linewidths=1, ax=axes[1], square=True, cbar=True)\n",
    "    axes[1].set_title('Kendall Correlation')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'fil/Sideby Correlation Comparisons {selection}.png') \n",
    "    \n",
    "show_correlation_matrix_method_comparisons(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = \"pearson\"\n",
    "sns.heatmap(data.corr(method=method_name), linewidths=1, annot=False, cmap='coolwarm')\n",
    "plt.title(f'Reordered {method_name} correlation matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extend the data by adding the next row as additional columns, with column names with \"next_\" prefix\n",
    "def extend_data_with_next_row(dataframe):\n",
    "    next_row = dataframe.shift(-1)\n",
    "    next_row.columns = [f\"next_{col}\" for col in next_row.columns]\n",
    "    return pd.concat([dataframe, next_row], axis=1).iloc[:-1]\n",
    "\n",
    "data_extended = extend_data_with_next_row(data)\n",
    "\n",
    "sns.heatmap(get_corr_matrix(data.diff(), method_name), linewidths=1, annot=False, cmap='coolwarm')\n",
    "plt.title(f'{method_name} correlation matrix with next step')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = data.corr(method=\"spearman\")\n",
    "\n",
    "def replace_correlation(value):\n",
    "    \"\"\"Excludes the values over the diagonals\"\"\"\n",
    "    if abs(value - 1.0) <= 0.01:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "def print_significance_report(correlation_matrix, thresh):\n",
    "    correlation_meaning = analyze_significance(correlation_matrix, thresh)\n",
    "    \n",
    "    print(correlation_meaning.to_string())\n",
    "    \n",
    "def exclude_self_correlated_values(correlation_matrix):\n",
    "    return correlation_matrix.apply(lambda column: column.apply(replace_correlation))\n",
    "\n",
    "correlation_matrix_without_self = exclude_self_correlated_values(correlation)\n",
    "print_significance_report(correlation_matrix_without_self, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def show_correlation_matrix_heatmap(correlation_matrix):\n",
    "    z_normalised_correlation = z_score_columns(correlation_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(z_normalised_correlation, annot=True, cmap='coolwarm', center=0)\n",
    "    \n",
    "    num_columns = z_normalised_correlation.shape[1]\n",
    "    for i in range(num_columns):\n",
    "        ax.add_patch(Rectangle((i, 0), 1, num_columns, fill=False, edgecolor='black', lw=2))\n",
    "        \n",
    "    ax.add_patch(Rectangle((0, 0), num_columns, num_columns, fill=False, edgecolor='black', lw=5))\n",
    "    plt.tight_layout()\n",
    "    plt.title('Correlation Matrix Heatmap, Normalised over the columns')\n",
    "    plt.savefig(f'fil/Correlation Matrix Heatmap Normalised over the columns {selection}.png') \n",
    "\n",
    "show_correlation_matrix_heatmap(correlation_matrix_without_self)\n",
    "# interpretation: the row represents an activity causing an effect on the column\n",
    "# the number is computed over the columns and represents the effect size, relative to the other rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_label(cluster, include_only_first_column = True):\n",
    "    label = \"\"\n",
    "    if include_only_first_column:\n",
    "        label += f\"{cluster_meaning.loc[cluster].iloc[0]}\".replace(\"'\", \"\")\n",
    "    else:\n",
    "        for col in cluster_meaning.columns:\n",
    "            if cluster_meaning.loc[cluster, col] != []:\n",
    "                label += f\"{col} {cluster_meaning.loc[cluster, col]} \"\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(pca):\n",
    "    Z = linkage(pca, method=\"ward\")\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    dendrogram(Z)\n",
    "    plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "    plt.tick_params(\n",
    "        axis=\"x\", bottom=False, labelbottom=False\n",
    "    )  # Turn off x-axis ticks and labels\n",
    "    plt.ylabel(\"Distance\")\n",
    "    plt.show()\n",
    "\n",
    "if rawDF.shape[0] < 1000:\n",
    "    plot_dendrogram(p_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_cmap = ListedColormap([\"purple\", \"blue\", \"brown\", \"green\", \"red\", \"gray\", \"orange\", \"black\", \"yellow\", \"pink\"])\n",
    "\n",
    "def plot_PCA_2D(pca, labels):\n",
    "    scatter = plt.scatter(pca[\"PC1\"], \n",
    "                          pca[\"PC2\"], c=labels, cmap=custom_cmap)\n",
    "    legend_labels = [get_cluster_label(i) for i in labels.unique()]\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=legend_labels, title=\"Cluster Labels\", loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()\n",
    "\n",
    "plot_PCA_2D(p_comp, df_cl[\"cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PCA_3D(pca, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    # draws every point\n",
    "    for i in range(len(np.unique(labels))):\n",
    "        ax.scatter(\n",
    "            pca[labels == i][\"PC1\"],\n",
    "            pca[labels == i][\"PC2\"],\n",
    "            pca[labels == i][\"PC3\"],\n",
    "            c=[custom_cmap(i)] * np.sum(labels == i),\n",
    "            label=get_cluster_label(i),\n",
    "            s=10,\n",
    "        )\n",
    "\n",
    "    ax.legend(loc='center left', bbox_to_anchor=(1.15, 0.5))\n",
    "    # Labeling axes\n",
    "    ax.set_xlabel(\"PC1\")\n",
    "    ax.set_ylabel(\"PC2\")\n",
    "    ax.set_zlabel(\"PC3\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_PCA_3D(p_comp, df_cl[\"cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequencies(dates_array, lam, columns):\n",
    "    \"\"\"lam must be a function that takes in a string date and\n",
    "    parses it to an integer number [1, bins]\"\"\"\n",
    "    cfreqs = []\n",
    "    bins = len(columns)\n",
    "    # i represents the cluster name\n",
    "    for i, C in enumerate(dates_array):\n",
    "        # extract day of the week\n",
    "        elw = pd.to_datetime(C).apply(lam)\n",
    "        elw = elw.value_counts() / len(C)\n",
    "\n",
    "        # expands the index to include days which don't figure\n",
    "        elw = elw.reindex(range(1, bins + 1), fill_value=0)\n",
    "\n",
    "        # sets the name of the column to the cluster\n",
    "        elw = elw.rename(i)\n",
    "\n",
    "        cfreqs.append(elw)\n",
    "\n",
    "    freq_df = pd.concat(cfreqs, axis=1).T\n",
    "    freq_df.columns = columns\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_series(x_labels, cfreq, description=\"Series Histogram\"):\n",
    "    number_of_clusters = len(cfreq)\n",
    "    fig, axs = plt.subplots(\n",
    "        number_of_clusters, 1, figsize=(5, number_of_clusters), sharex=True\n",
    "    )\n",
    "    fig.suptitle(description)\n",
    "    for i, series in cfreq.iterrows():\n",
    "        # Define bin edges to align with the ticks\n",
    "        bin_edges = np.arange(len(series) + 1) - 0.4\n",
    "\n",
    "        # Create a histogram for each series\n",
    "        axs[i].hist(np.arange(len(series)), weights=series, bins=bin_edges, width=0.8)\n",
    "        axs[i].set_title(get_cluster_label(i), loc=\"left\")\n",
    "        axs[i].set_ylabel(\"Frequency\")\n",
    "        axs[i].set_xticks(range(len(series)))  # Set tick positions for x-axis\n",
    "        axs[i].set_xticklabels(x_labels)\n",
    "        axs[i].tick_params(axis=\"x\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_histogram(dataframe_with_clusters, bin_extractor_from_date, title_description, bin_titles):\n",
    "    cluster_grouping = dataframe_with_clusters.drop(columns=[\"from_time\", \"to_time\"]).groupby(\"cluster\")\n",
    "    \n",
    "    start_time_by_cluster = [df.iloc[cluster.index][\"from_time\"] for _, cluster in cluster_grouping]\n",
    "\n",
    "    frequencies = extract_frequencies(start_time_by_cluster, bin_extractor_from_date, bin_titles)\n",
    "    plot_histogram_series(bin_titles, frequencies, \"Distribution over the \" + title_description)\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hour(x):\n",
    "    \"\"\"Extracts the hour 2 hour block from a datetime object\"\"\"\n",
    "    return int(x.strftime(\"%H\"))//2\n",
    "\n",
    "if time_step.total_seconds() // 3600 < 24:\n",
    "    distribution_day = plot_histogram(df_cl, get_hour, \"2h blocks\", range(0, 12))\n",
    "    # print(distribution_day.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week_labels = [\"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\", \"SUN\"]\n",
    "\n",
    "def get_weekday(x):\n",
    "    A = int(x.strftime(\"%w\"))\n",
    "    return 7 if A == 0 else A\n",
    "\n",
    "if time_step.days < 7:\n",
    "    distribution_week = plot_histogram(df_cl, get_weekday, \"week\", week_labels)\n",
    "    # print(distribution_week.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_labels = [\"JAN\", \"FEB\", \"MAR\", \"APR\", \"MAY\", \"JUN\", \"JUL\", \"AUG\", \"SEP\", \"OCT\", \"NOV\", \"DEC\"]\n",
    "\n",
    "def get_month(x):\n",
    "    return int(x.strftime(\"%m\"))\n",
    "        \n",
    "distribution_month = plot_histogram(df_cl, get_month, \"month\", month_labels)\n",
    "#Â print(distribution_month.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_transition_data(dataframe):\n",
    "    series = dataframe[\"cluster\"]\n",
    "    transitions = pd.DataFrame({'from': list(series[:-1]), 'to': list(series[1:])})\n",
    "    return transitions\n",
    "\n",
    "def plot_transition_matrix(dataframe):\n",
    "    transitions = get_transition_data(dataframe)\n",
    "    transition_counts = transitions.groupby(['from', 'to']).size().unstack(fill_value=0)\n",
    "    transition_matrix = transition_counts.div(transition_counts.sum(axis=1), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(transition_matrix, annot=True, cmap=\"Blues\", fmt=\".2f\", cbar=True)\n",
    "    plt.title(\"Transition Matrix\", fontsize=16)\n",
    "    \n",
    "    plt.xlabel(\"To\", fontsize=14)\n",
    "    plt.ylabel(\"From\", fontsize=14)\n",
    "    \n",
    "    labels = [get_cluster_label(i) for i in transition_matrix.columns]\n",
    "    ticks = [i + 0.5 for i in range(len(transition_matrix.columns))]\n",
    "    \n",
    "    plt.xticks(ticks=ticks, labels=labels)\n",
    "    plt.yticks(ticks=ticks, labels=labels)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def restrict_df_to_date_range(dataframe, from_date, to_date):\n",
    "    return dataframe[(dataframe[\"from_time\"] >= from_date) & (dataframe[\"from_time\"] < to_date)]\n",
    "\n",
    "# early year university session\n",
    "from_date = pd.to_datetime(\"2024-02-01\", utc=True)\n",
    "to_date = pd.to_datetime(\"2024-06-01\", utc=True)\n",
    "\n",
    "restr_df_cl = restrict_df_to_date_range(df_cl, from_date, to_date)\n",
    "plot_transition_matrix(df_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def do_time_data_split(dataframe):\n",
    "    X = dataframe[:-1]\n",
    "    y = dataframe.shift(-1).dropna()\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def try_linear_regression(dataframe):\n",
    "    X_train, X_test, y_train, y_test = do_time_data_split(dataframe)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return mse, model.intercept_, model.coef_, r2\n",
    "\n",
    "def print_linear_model(vars):\n",
    "    mse, intercept, coef, r2 = vars\n",
    "    print(f'Mean Squared Error: {mse}')\n",
    "    print(f'Intercept: {intercept}')\n",
    "    print(f'Coefficient: {coef}')\n",
    "    print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "def compare_linear_models_datasets(data1, data2):\n",
    "    model1 = try_linear_regression(data1)\n",
    "    mse1 = model1[0]\n",
    "    r2_1 = model1[3]\n",
    "    \n",
    "    model2 = try_linear_regression(data2)\n",
    "    mse2 = model2[0]\n",
    "    r2_2 = model2[3]\n",
    "    \n",
    "    print(f\"% delta mse {(2*(mse2 - mse1) / (mse1 + mse2)):.4f}\")\n",
    "    \n",
    "    print(f\"min mse {min(mse1, mse2):.4f}\")\n",
    "    print(\"max r2\", max(r2_1, r2_2))\n",
    "    \n",
    "def create_dataframe_with_cluster_column(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    dataframe[\"cluster\"] = df_cl[\"cluster\"]\n",
    "    return dataframe\n",
    "\n",
    "# data is first normalised\n",
    "scaled_data_df = to_normalised_df(data)\n",
    "\n",
    "scaled_data_df_cl = create_dataframe_with_cluster_column(scaled_data_df)\n",
    "l_reg = try_linear_regression(scaled_data_df)\n",
    "print(\"R^2 coefficient:\", l_reg[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_normalise(data_dataframe):\n",
    "    # drops the columns with all 0 values\n",
    "    kept_and_nonempty_columns = (data_dataframe != 0).any(axis=0)\n",
    "    original_data = data_dataframe.loc[:, kept_and_nonempty_columns]\n",
    "    return to_normalised_df(original_data)\n",
    "\n",
    "def plot_plausible_energy_distributions(filter_and_normalised_data, verbose=False):\n",
    "    corr_matrix = filter_and_normalised_data.corr()\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(corr_matrix)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nEigenvalues:\")\n",
    "        print(eigenvalues)\n",
    "\n",
    "        print(\"\\nMinimum Eigenvalue:\")\n",
    "        print(eigenvalues[0])\n",
    "\n",
    "        print(\"\\nCorresponding Eigenvector (normalized):\")\n",
    "        print(eigenvectors[0] / np.linalg.norm(eigenvectors[0]))\n",
    "\n",
    "    eigenvectors_df = pd.DataFrame(eigenvectors.T, columns=filter_and_normalised_data.columns)\n",
    "    grid = plt.GridSpec(len(eigenvectors_df), 2, width_ratios=[3, 1])\n",
    "\n",
    "    ax1 = plt.subplot(grid[:, 0])\n",
    "    sns.heatmap(eigenvectors_df, cmap=\"coolwarm\", center=0, fmt=\".2f\", annot=True, \n",
    "                cbar=False, \n",
    "                ax=ax1)\n",
    "\n",
    "    explained_variance_labels = [f\"{lam:.2f}\" for lam in eigenvalues]\n",
    "    explained_variance_labels[0] = \"Var[c*x] = \" + explained_variance_labels[0]\n",
    "\n",
    "    ax1.set_yticklabels(explained_variance_labels, rotation=0, fontsize=8, fontweight='bold')\n",
    "\n",
    "    energy_values = pd.DataFrame({i:filter_and_normalised_data @ eigenvectors_df.iloc[i].to_numpy() for i in range(len(eigenvectors_df))})\n",
    "    \n",
    "    x_max_value = energy_values.max(axis=None)\n",
    "    x_min_value = energy_values.min(axis=None)\n",
    "    \n",
    "    for i in range(len(eigenvectors_df)):\n",
    "        ax2 = plt.subplot(grid[i, 1])\n",
    "        \n",
    "        ax2.hist(energy_values[i], bins=20, orientation='vertical', color='blue')\n",
    "        # set the axis labels small\n",
    "        ax2.tick_params(axis='both', which='major', labelsize=5)\n",
    "                \n",
    "        ax2.set_xlim(x_min_value, x_max_value)\n",
    "        \n",
    "        if i == 0:\n",
    "            ax1.set_title('Heatmap for each interpretation')\n",
    "            ax2.set_title('Histogram of energies')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return (filter_and_normalised_data @ eigenvectors_df.iloc[0].to_numpy(), eigenvectors_df)\n",
    "    \n",
    "eigevectors_dict = {}\n",
    "eigevectors_dict[\"general\"] = plot_plausible_energy_distributions(filter_and_normalise(data))\n",
    "eigevectors_dict[\"lessons\"] = plot_plausible_energy_distributions(filter_and_normalise(extract_data_from_df(df[get_df_period_mask(\"lessons\", df)])))\n",
    "eigevectors_dict[\"exams\"] = plot_plausible_energy_distributions(filter_and_normalise(extract_data_from_df(df[get_df_period_mask(\"exams\", df)])))\n",
    "eigevectors_dict[\"university\"] = plot_plausible_energy_distributions(filter_and_normalise(extract_data_from_df(df[get_df_period_mask(\"university\", df)])))\n",
    "eigevectors_dict[\"not_university\"] = plot_plausible_energy_distributions(filter_and_normalise(extract_data_from_df(df[~get_df_period_mask(\"university\", df)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(dataframe):\n",
    "    # force square shape\n",
    "    ax = sns.heatmap(dataframe.T, annot=True, cmap='coolwarm', center=0, fmt=\".2f\", square=True, cbar=False)\n",
    "    ax.set_title(\"Heatmap of the Energy Distribution\")\n",
    "    ax.set_xlabel(\"Activity\")\n",
    "    ax.set_ylabel(\"Eigenvector\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "eigenvectors_list = []\n",
    "for key, (energy_values, eigenvectors_df) in eigevectors_dict.items():\n",
    "    # only takes the best estimate\n",
    "    eigenvectors_list.append(eigenvectors_df.iloc[0])\n",
    "    \n",
    "merged_eigenvectors = pd.concat(eigenvectors_list, axis=1, keys=[key for key in eigevectors_dict.keys()])\n",
    "\n",
    "plot_heatmap(merged_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "n_components = 2\n",
    "\n",
    "presets = {\n",
    "    \"weekly\": {},\n",
    "    \"daily\": {\n",
    "        \"UMAP\":{\n",
    "            # NOTE: transformed_data = data.div(1 + data.sum(axis=1), axis=0)\n",
    "            \"n_neighbors\": 50,\n",
    "            \"min_dist\": 0,\n",
    "        }\n",
    "    },\n",
    "    \"4h\": {\n",
    "        \"UMAP\":{\n",
    "            \"n_neighbors\": 70,\n",
    "            \"min_dist\": 0.1,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=1000,\n",
    "    min_dist=0,\n",
    "    n_components = n_components\n",
    ")\n",
    "# divide each row by sum over row\n",
    "# transformed_data = data.div(1 + data.sum(axis=1), axis=0)\n",
    "transformed_data = data.div(1 + data.sum(axis=1), axis=0)\n",
    "embedding = pd.DataFrame(reducer.fit_transform(transformed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks with index 0 the \"general\" energy function\n",
    "def plot(data_embedding, n_components, labelling_values, title, cbar=True):\n",
    "    # filters the embedding data to ones whos index appears in labelling_values\n",
    "    data_embedding = data_embedding.iloc[labelling_values.index]\n",
    "    if n_components == 2:\n",
    "        plt.figure()\n",
    "        scatter = plt.scatter(data_embedding.iloc[:, 0], data_embedding.iloc[:, 1], c=labelling_values, s=5)\n",
    "        if cbar:\n",
    "            plt.colorbar(label='Label')\n",
    "        plt.xlabel('UMAP 1')\n",
    "        plt.ylabel('UMAP 2')\n",
    "        plt.title(f'UMAP Projection {title}')   \n",
    "        return scatter\n",
    "    else:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(data_embedding[:, 0], data_embedding[:, 1], data_embedding[:, 2], c=labelling_values, s=5)\n",
    "        ax.set_xlabel('UMAP 1')\n",
    "        ax.set_ylabel('UMAP 2')\n",
    "        ax.set_zlabel('UMAP 3')\n",
    "        ax.set_title(f'UMAP Projection {title}')\n",
    "        plt.show()\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_of_day = df[\"from_time\"].dt.hour + df[\"from_time\"].dt.minute / 60\n",
    "month_of_year = df[\"from_time\"].dt.month\n",
    "day_of_week = df[\"from_time\"].dt.dayofweek\n",
    "year = df[\"from_time\"].dt.year\n",
    "energy = eigevectors_dict[\"lessons\"][0].abs()\n",
    "\n",
    "df_period_extended = get_all_period_masks(df)\n",
    "for key in df_period_extended.keys():\n",
    "    plot(embedding, n_components, df_period_extended[key], key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "if n_components==2:\n",
    "    # umap.plot.diagnostic(reducer, diagnostic_type='local_dim')\n",
    "    umap.plot.diagnostic(reducer, diagnostic_type='vq')\n",
    "    umap.plot.diagnostic(reducer, diagnostic_type='pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_data_columns_over_time(n, data):\n",
    "    # Group the data into chunks of size n by summing the columns\n",
    "    df_grouped = data.groupby(np.arange(len(df)) // n).sum().div(n)\n",
    "    columns = data.columns\n",
    "    # Create a new 'from_time' column for the grouped DataFrame to plot against\n",
    "    df_grouped['from_time'] = df['from_time'][::n].reset_index(drop=True)\n",
    "\n",
    "    # Plot the value of one of the columns (e.g., 'Projects') on the y-axis and the time on the x-axis\n",
    "    for column in columns:\n",
    "        plt.figure()\n",
    "        plt.plot(df_grouped['from_time'], df_grouped[column], label=column)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel(column)\n",
    "        plt.title(f'{column} over Time')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "def plot_a_data_columns_over_time(n, data, column):\n",
    "    # Group the data into chunks of size n by summing the columns\n",
    "    df_grouped = data.groupby(np.arange(len(df)) // n).sum().div(n)\n",
    "    columns = data.columns\n",
    "    # Create a new 'from_time' column for the grouped DataFrame to plot against\n",
    "    df_grouped['from_time'] = df['from_time'][::n].reset_index(drop=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(df_grouped['from_time'], df_grouped[column], label=column)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(column)\n",
    "    plt.title(f'{column} over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def display_all_data_columns_over_time(n, data):\n",
    "    # Group the data into chunks of size n by summing the columns\n",
    "    df_grouped = data.groupby(np.arange(len(df)) // n).sum()\n",
    "\n",
    "    # Create a new 'from_time' column for the grouped DataFrame to plot against\n",
    "    df_grouped['from_time'] = df['from_time'][::n].reset_index(drop=True)\n",
    "\n",
    "    # Divide the grouped data by n\n",
    "    for column in df_grouped.columns:\n",
    "        if column != 'from_time':\n",
    "            df_grouped[column] = df_grouped[column] / n\n",
    "\n",
    "    # List of columns to plot (excluding 'from_time')\n",
    "    columns = df_grouped.columns[df_grouped.columns != 'from_time']\n",
    "\n",
    "    # Plot all columns on the same figure\n",
    "    plt.figure()\n",
    "    for column in columns:\n",
    "        plt.plot(df_grouped['from_time'], df_grouped[column], label=column)\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Average value per step')\n",
    "    plt.title('Columns over Time')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "modified_data = df.drop(columns=[\"from_time\", \"to_time\", \"Sleep\"])\n",
    "modified_data[\"Lessons\"] = df[\"Lessons\"] + df[\"Revision\"]\n",
    "modified_data.drop(columns=[\"Revision\"], inplace=True)\n",
    "\n",
    "display_all_data_columns_over_time(30, modified_data)\n",
    "\n",
    "plot_a_data_columns_over_time(7, data, \"Projects\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
